# -*- coding: utf-8 -*-
"""DAPL_Project_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zew7I_gvs-DhOwpgO-b2djnb-FSt82sY
"""

# Commented out IPython magic to ensure Python compatibility.
#Spring 2022 Project - Echo Ridge and Dartmouth MEM
#Analysis of Invisage Data to derive investment signal

from pathlib import Path 
import pathlib
import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
# %config InlineBackend.figure_formats = ['svg']
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder



import pandas
from pandas import read_csv
from keras.models import Sequential
from keras.layers import Dense
from scikeras.wrappers import KerasClassifier, KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn import tree

#Loading the datasets

coredata = pd.read_csv (r'C:\Users\sanja\OneDrive\Desktop\daplproject\coredata.csv')   #read the csv file 
premiumdata = pd.read_csv (r'C:\Users\sanja\OneDrive\Desktop\daplproject\premiumdata.csv')

#Correlation plot with core dataset

corr = coredata.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation plot with premium dataset

corr = premiumdata.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Consider removing top analyst factor value and performance

#Correlation between a few features in the combined dataset (core and premium datasets) 

combineddata = pd.read_csv (r'C:\Users\sanja\OneDrive\Desktop\daplproject\Combineddata.csv')
df = combineddata.iloc[:, 10:14]
corr = df.corr()
sns.heatmap(corr, annot=True)
plt.show()
combineddata.dtypes

#Part I - Exploratory Data Analysis 

#Correlations are done for periods one week, one month, 3 months, 6 months, 1 year and 3 years for 2013, 2015 and 2017

#Correlation with one week - 2013

combineddata['date'] = pd.to_datetime(combineddata['date'])
combineddata = combineddata.sort_values(by='date', ascending = 'TRUE')  # Sorted by date
Week_1_data = combineddata.loc[(combineddata['date'] >= '2013-02-04') & (combineddata['date'] < '2013-02-09')]
Week_1_data = Week_1_data.iloc[:, 10:14]
corr = Week_1_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with one month - 2013

Month_1_data = combineddata.loc[(combineddata['date'] >= '2013-02-01') & (combineddata['date'] < '2013-03-01')]
Month_1_data = Month_1_data.iloc[:, 10:14]
corr = Month_1_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with three months - 2013 

ThreeMonths_data = combineddata.loc[(combineddata['date'] >= '2013-02-01') & (combineddata['date'] < '2013-05-01')]
ThreeMonths_data = ThreeMonths_data.iloc[:, 10:14]
corr = ThreeMonths_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with six months - 2013
SixMonths_data = combineddata.loc[(combineddata['date'] >= '2013-02-01') & (combineddata['date'] < '2013-08-01')]
SixMonths_data = SixMonths_data.iloc[:, 10:14]
corr = SixMonths_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with a year - 2013

OneYear_data = combineddata.loc[(combineddata['date'] >= '2013-02-01') & (combineddata['date'] < '2014-02-01')]
OneYear_data = OneYear_data.iloc[:, 10:14]
corr = OneYear_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with three years - 2013

ThreeYear_data = combineddata.loc[(combineddata['date'] >= '2013-02-01') & (combineddata['date'] < '2018-02-01')]
ThreeYear_data = ThreeYear_data.iloc[:, 10:14]
corr = ThreeYear_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with one week - 2015
Week_1_data = combineddata.loc[(combineddata['date'] >= '2015-02-02') & (combineddata['date'] < '2015-02-07')]
Week_1_data = Week_1_data.iloc[:, 10:14]
corr = Week_1_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with one month - 2015
Month_1_data = combineddata.loc[(combineddata['date'] >= '2015-02-01') & (combineddata['date'] < '2015-03-01')]
Month_1_data = Month_1_data.iloc[:, 10:14]
corr = Month_1_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with three months 2015
ThreeMonths_data = combineddata.loc[(combineddata['date'] >= '2015-02-01') & (combineddata['date'] < '2015-05-01')]
ThreeMonths_data = ThreeMonths_data.iloc[:, 10:14]
corr = ThreeMonths_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with six months 2015
SixMonths_data = combineddata.loc[(combineddata['date'] >= '2015-02-01') & (combineddata['date'] < '2015-08-01')]
SixMonths_data = SixMonths_data.iloc[:, 10:14]
corr = SixMonths_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with a year 2015
OneYear_data = combineddata.loc[(combineddata['date'] >= '2015-02-01') & (combineddata['date'] < '2016-02-01')]
OneYear_data = OneYear_data.iloc[:, 10:14]
corr = OneYear_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with one week- 2017
Week_1_data = combineddata.loc[(combineddata['date'] >= '2017-05-01') & (combineddata['date'] < '2017-05-06')]
Week_1_data = Week_1_data.iloc[:, 10:14]
corr = Week_1_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with one month 2017
Month_1_data = combineddata.loc[(combineddata['date'] >= '2017-02-01') & (combineddata['date'] < '2017-03-01')]
Month_1_data = Month_1_data.iloc[:, 10:14]
corr = Month_1_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with three months 2017
ThreeMonths_data = combineddata.loc[(combineddata['date'] >= '2017-02-01') & (combineddata['date'] < '2017-05-01')]
ThreeMonths_data = ThreeMonths_data.iloc[:, 10:14]
corr = ThreeMonths_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with six months 2017
SixMonths_data = combineddata.loc[(combineddata['date'] >= '2017-02-01') & (combineddata['date'] < '2017-08-01')]
SixMonths_data = SixMonths_data.iloc[:, 10:14]
corr = SixMonths_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

#Correlation with a year 2017
OneYear_data = combineddata.loc[(combineddata['date'] >= '2017-02-01') & (combineddata['date'] < '2018-02-01')]
OneYear_data = OneYear_data.iloc[:, 10:14]
corr = OneYear_data.corr()
sns.heatmap(corr, annot=True)
plt.show()

# Part II - Price Prediction Model

#Model 1 - Deep Neural Network

#Feature engineering
#Dropping symbol column from combinedddata
data = combineddata.drop(columns = "symbol")

#One hot encoding 
data_one_hot_security_name: pd.DataFrame = pd.get_dummies(data, columns = ['security_name']) 
newdata: pd.DataFrame = pd.get_dummies(data_one_hot_security_name, columns = ['top_analyst_name'])

#Encoding date
newdata["year"] = newdata["date"].dt.year
newdata["month"] = newdata["date"].dt.month
newdata["day"] = newdata["date"].dt.day
newdata["week"] = newdata["date"].dt.isocalendar().week
data = newdata.drop(columns = "date")
data

#Converting data types
data = data.astype({'day':'float'})
data = data.astype({'week':'float'})
data = data.astype({'year':'float'})
data = data.astype({'month':'float'})
data = data.astype({'No.of analysts':'float'})
data

# Deep Neural Network Code

from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense

# load the dataset

data = data.dropna()
data = data.reset_index(drop = 'TRUE')

x = data.drop(["close_price","open_price","day","month","week","top_analyst_factor_value"], axis=1)
y = data["close_price"]

from sklearn.model_selection import train_test_split

# split into train test sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
print(X_train)

#Model 2 - Simple Linear Regression 

#Variables with p-values greater than 0.05 are eliminated and the model is run again until all 
#p-values are less than 5% significance

import statsmodels.api as sm
model = sm.OLS(y_train, X_train).fit()
model.summary()

#Predict
predictions= model.predict(X_test)

#Accuracy
mse = mean_squared_error(y_test,predictions)
print(mse)


#Continuation of Model 1 - Deep Neural Network

#Initialising the Neural Network
regressor = Sequential()

# Adding the input layer and the first hidden layer
regressor.add(Dense (units = 25, activation = "ReLU",input_dim = 472))

#Adding the second hidden layer
regressor.add(Dense(units = 25, activation = "ReLU"))

# Adding the output layer
regressor.add(Dense(units = 1))

# Compiling the ANN
regressor.compile(optimizer = "adam", loss = "mean_squared_error")

# Fitting the ANN to the Training set
regressor.fit(X_train, y_train,batch_size = 5,epochs = 100)

print(y_test)

predictions = regressor.predict(X_test)
print(predictions)

#Accuracy 

acc = regressor.evaluate(X_test,y_test)
print(acc)

#Model 3 - Random_Forest_Model

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.ensemble import RandomForestRegressor

regressor2 = RandomForestRegressor(n_estimators = 100, random_state = 0)

regressor2.fit(X_train, y_train)

print(y_test)

predictions = regressor2.predict(X_test)
print(predictions)

#Accuracy 

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, predictions)
mse

# Model 4  

from sklearn.pipeline import make_pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.decomposition import PCA

# Hyperparameters for GradientBoostingRegressor
gbr_params = {'n_estimators': 1000,
          'max_depth': 3,
          'min_samples_split': 5,
          'learning_rate': 0.1}

# Create an instance of gradient boosting regressor
gbr = GradientBoostingRegressor(**gbr_params)

# Fit the model
gbr.fit(X_train, y_train)

predictions = gbr.predict(X_test)
mse = mean_squared_error(y_test, predictions)
mse

#We can see from the mean square error (mse) values that mse is lowest for random forest model 
#Thus, we will be using the random forest model for price prediction

# Part III - Ranking Analysts

#Using the random forest price prediction model developed, the predictions of individual analysts are identified 
#by subsetting the dataset
#The analysts are ranked based on highest accuracy or lowest mse values obtained from the model

#Testing model with one analyst
newdata=data[data['top_analyst_name_Andrew Kaplowitz'] == 1]
x = newdata.drop(["close_price","open_price","day","month","week","top_analyst_factor_value"], axis=1)
y = newdata["close_price"].values
predictions = regressor2.predict(x)
mse = mean_squared_error(y, predictions)
mse

#Testing model with another analyst
newdata=data[data['top_analyst_name_A. Kligerman'] == 1]
x = newdata.drop(["close_price","open_price","day","month","week","top_analyst_factor_value"], axis=1)
y = newdata["close_price"].values
predictions = regressor2.predict(x)
mse = mean_squared_error(y, predictions)
mse

# Running the model with all the analyst and ranking the analysts

mse_values = {}
for i in range(61,474):
  newdata=data[data.iloc[:,i] == 1]
  x = newdata.drop(["close_price","open_price","day","month","week","top_analyst_factor_value"], axis=1)
  y = newdata["close_price"].values
  predictions = regressor2.predict(x)
  mse = mean_squared_error(y, predictions)
  mse_values[newdata.columns[i]] = mse

df_dict = pd.DataFrame(dict(sorted(mse_values.items(), key=lambda item: item[1])).items(), columns=["Analyst Name", "mse"])

print ("{:<50} {:<50}".format('Analyst_name','mse'))
for k, v in dict(sorted(mse_values.items(), key=lambda item: item[1])).items():
    mse = v
    print ("{:<50} {:<50}".format(k, mse))

# Part IV - Ranking Stocks 

#Using the random forest price prediction model developed, the predictions of stocks are identified 
#by subsetting the dataset
#The stocks are ranked based on highest accuracy or lowest mse values obtained from the model

mse_values = {}
for i in range(10,61):
  newdata=data[data.iloc[:,i] == 1]
  x = newdata.drop(["close_price","open_price","day","month","week","top_analyst_factor_value"], axis=1)
  y = newdata["close_price"].values
  predictions = regressor2.predict(x)
  mse = mean_squared_error(y, predictions)
  mse_values[newdata.columns[i]] = mse

df_dict = pd.DataFrame(dict(sorted(mse_values.items(), key=lambda item: item[1])).items(), columns=["Stock Name", "mse"])
df_dict
